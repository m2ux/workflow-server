id: implementation-analysis
version: 1.0.0
title: Implementation Analysis Guide
purpose: Guidelines for analyzing the existing implementation during work package planning to establish baselines, evaluate effectiveness, and identify improvement opportunities

sections[9]:
  - id: overview
    title: Overview
    content: """
      Before designing a solution, analyze the existing implementation to understand:
      - **Existing Usage** - How is the feature/component used today?
      - **Effectiveness** - Is it working well? What evidence exists?
      - **Baseline Metrics** - What are current performance/quality measurements?
      - **Gaps** - What's missing, broken, or underperforming?
      - **Opportunities** - Where can improvements have the most impact?
      - **Success Criteria** - What quantitative targets should the work package achieve?

      > **Key Insight:** You can't improve what you don't measure. Establishing baselines before implementation enables objective validation of improvements.
    """

  - id: when-to-apply
    title: When to Apply This Guide
    alwaysAnalyzeWhen[4]:
      - Work package modifies existing functionality
      - Performance improvements are expected
      - Quality metrics are defined
      - Comparison before/after is needed
    lightweightAcceptableWhen[3]:
      - Greenfield implementation (no existing code)
      - Simple bug fix with obvious solution
      - Documentation-only changes

  - id: analysis-framework
    title: Analysis Framework
    steps[6]:
      - id: implementation-review
        name: Implementation Review
        intro: "Understand where and how the feature/component is used:"
        areas[5]{area,questions}:
          Location,Where is the code? What files/modules?
          Usage,How is it called? What triggers it?
          Dependencies,What does it depend on? What depends on it?
          Architecture,What patterns are used? How is it structured?
          Integration,How does it connect to other components?
        methods[4]:
          - Code search for function/class usage
          - Trace call paths
          - Review imports and exports
          - Examine configuration

      - id: effectiveness-evaluation
        name: Effectiveness Evaluation
        intro: "Gather evidence of existing performance:"
        evidenceTypes[5]{type,whatToLookFor}:
          Logs,"Error rates, latency, throughput"
          Metrics,"Dashboard data, monitoring alerts"
          Tests,"Pass rates, coverage gaps, flaky tests"
          Issues,"Bug reports, user complaints"
          Comments,"TODO notes, workarounds, technical debt"
        keyQuestions[4]:
          - "What's working well? (with evidence)"
          - "What's not working? (with evidence)"
          - Are there workarounds in place?
          - What do error logs show?

      - id: baseline-metrics
        name: Baseline Metrics
        intro: "Establish quantitative measurements:"
        categories[4]{category,examples}:
          Performance,"Latency (P50, P95, P99), throughput, memory usage"
          Quality,"Error rates, success rates, test coverage"
          Usage,"Call frequency, user adoption, feature utilization"
          Reliability,"Uptime, failure rate, recovery time"
        note: "Document how each metric was measured for reproducibility."

      - id: gap-analysis
        name: Gap Analysis
        intro: "Compare existing state to desired state:"
        gapTypes[4]{type,questions}:
          Functional,What capabilities are missing?
          Performance,Where does it fall short of targets?
          Quality,What defects or issues exist?
          Maintainability,What makes the code hard to change?
        prioritization[3]:
          - "HIGH: Blocking or critical impact"
          - "MEDIUM: Significant but not blocking"
          - "LOW: Nice to have"

      - id: opportunity-identification
        name: Opportunity Identification
        intro: "Identify where improvements can have impact:"
        types[4]{type,considerations}:
          Quick Wins,"Low effort, immediate benefit"
          Structural,"Larger effort, foundational improvement"
          Optimization,Performance-focused refinements
          Cleanup,Technical debt reduction

      - id: success-criteria
        name: Success Criteria Definition
        format: "Improve [metric] from [baseline] to [target] ([X]% improvement)"
        examples[3]:
          - "Reduce P95 latency from 500ms to 200ms (60% reduction)"
          - "Increase test coverage from 65% to 90% (25 percentage points)"
          - "Reduce error rate from 5% to 1% (80% reduction)"

  - id: checklist
    title: Analysis Checklist
    items[8]:
      - Existing usage patterns documented
      - Code locations and dependencies mapped
      - Effectiveness evaluated with evidence
      - Baseline metrics established (with measurement method)
      - Gaps identified and prioritized
      - Opportunities listed with expected impact
      - Success criteria defined (quantitative)
      - Measurement strategy for validation defined

  - id: planning-artifact
    title: Planning Artifact
    intro: "Store analysis findings in a discrete planning document:"
    location: ".engineering/artifacts/planning/YYYY-MM-DD-work-package-name/01-implementation-analysis.md"
    template: workflow://work-package/templates/01

  - id: checkpoint
    title: Checkpoint Template
    intro: "After completing analysis, present findings to the user for confirmation."
    template: workflow://work-package/templates/12

  - id: quality-indicators
    title: Quality Indicators
    subsections[2]:
      - id: good-analysis
        title: Good Analysis
        indicators[5]:
          - "✅ Quantitative baselines with measurement methodology"
          - "✅ Evidence-based effectiveness claims"
          - "✅ Gaps linked to measurable success criteria"
          - "✅ Clear before/after comparison strategy"
          - "✅ Priorities justified with impact assessment"

      - id: insufficient-analysis
        title: Insufficient Analysis
        indicators[5]:
          - "❌ No baseline metrics"
          - "❌ Vague claims without evidence"
          - "❌ Success criteria not measurable"
          - "❌ Missing measurement strategy"
          - "❌ Gaps not prioritized"

  - id: integration
    title: Integration with Workflow
    steps[4]:
      - After KB research confirmed → Begin implementation analysis
      - Complete analysis → Store in 01-implementation-analysis.md
      - Present checkpoint → Get user confirmation
      - Proceed to design → Use baselines to inform success criteria

  - id: related-guides
    title: Related Guides
    guides[4]:
      - workflow://work-package/guides/work-package
      - workflow://work-package/guides/knowledge-base-research
      - workflow://work-package/guides/architecture-review
      - workflow://work-package/guides/test-plan
